{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn7PKu9r0asK"
      },
      "source": [
        "# Tutorial 8 - Options\n",
        "\n",
        "Please complete this tutorial to get an overview of options and an implementation of SMDP Q-Learning and Intra-Option Q-Learning.\n",
        "\n",
        "\n",
        "### References:\n",
        "\n",
        " [Recent Advances in Hierarchical Reinforcement\n",
        "Learning](https://people.cs.umass.edu/~mahadeva/papers/hrl.pdf) is a strong recommendation for topics in HRL that was covered in class. Watch Prof. Ravi's lectures on moodle or nptel for further understanding the core concepts. Contact the TAs for further resources if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uin285juQfA",
        "outputId": "09a21d68-48c8-42ba-90e7-bb22edda8e2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym==0.15.7 in /usr/local/lib/python3.10/dist-packages (0.15.7)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym==0.15.7) (1.11.4)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.15.7) (1.23.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gym==0.15.7) (1.16.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.15.7) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.15.7) (1.2.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.15.7) (0.18.3)\n",
            "Requirement already satisfied: numpy==1.23.1 in /usr/local/lib/python3.10/dist-packages (1.23.1)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install gym==0.15.7\n",
        "!pip3 install numpy==1.23.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "P_DODRgW_ZKS"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "A bunch of imports, you don't have to worry about these\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import gym\n",
        "# from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYNA5kiH_esJ",
        "outputId": "44703947-51f7-4974-b042-5cbc90b4e747"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "36\n",
            "Number of states: 48\n",
            "Number of actions that an agent can take: 4\n",
            "Action taken: left\n",
            "Transition probability: {'prob': 1.0}\n",
            "Next state: 36\n",
            "Reward recieved: -1\n",
            "Terminal state: False\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "x  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "The environment used here is extremely similar to the openai gym ones.\n",
        "At first glance it might look slightly different.\n",
        "The usual commands we use for our experiments are added to this cell to aid you\n",
        "work using this environment.\n",
        "'''\n",
        "\n",
        "#Setting up the environment\n",
        "from gym.envs.toy_text.cliffwalking import CliffWalkingEnv\n",
        "env = CliffWalkingEnv()\n",
        "\n",
        "env.reset()\n",
        "\n",
        "#Current State\n",
        "print(env.s)\n",
        "\n",
        "# 4x12 grid = 48 states\n",
        "print (\"Number of states:\", env.nS)\n",
        "\n",
        "# Primitive Actions\n",
        "action = [\"up\", \"right\", \"down\", \"left\"]\n",
        "#correspond to [0,1,2,3] that's actually passed to the environment\n",
        "\n",
        "# either go left, up, down or right\n",
        "print (\"Number of actions that an agent can take:\", env.nA)\n",
        "\n",
        "# Example Transitions\n",
        "rnd_action = random.randint(0, 3)\n",
        "print (\"Action taken:\", action[rnd_action])\n",
        "next_state, reward, is_terminal, t_prob= env.step(rnd_action)\n",
        "print (\"Transition probability:\", t_prob)\n",
        "print (\"Next state:\", next_state)\n",
        "print (\"Reward recieved:\", reward)\n",
        "print (\"Terminal state:\", is_terminal)\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apuaOxavDXus"
      },
      "source": [
        "#### Options\n",
        "We custom define very simple options here. They might not be the logical options for this settings deliberately chosen to visualise the Q Table better.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "g4MRC1p2DZbp",
        "outputId": "d35cecdd-e480-4cc0-e93c-2b2ae6c304a6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nNow the new action space will contain\\nPrimitive Actions: [\"up\", \"right\", \"down\", \"left\"]\\nOptions: [\"Away\",\"Close\"]\\nTotal Actions :[\"up\", \"right\", \"down\", \"left\", \"Away\", \"Close\"]\\nCorresponding to [0,1,2,3,4,5]\\n'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We are defining two more options here\n",
        "# Option 1 [\"Away\"] - > Away from Cliff (ie keep going up)\n",
        "# Option 2 [\"Close\"] - > Close to Cliff (ie keep going down)\n",
        "\n",
        "def Away(env,state):\n",
        "\n",
        "    optdone = False\n",
        "    optact = 0\n",
        "\n",
        "    if (int(state/12) == 0):\n",
        "        optdone = True\n",
        "\n",
        "    return [optact,optdone]\n",
        "\n",
        "def Close(env,state):\n",
        "\n",
        "    optdone = False\n",
        "    optact = 2\n",
        "\n",
        "    if (int(state/12) == 2):\n",
        "        optdone = True\n",
        "\n",
        "    if (int(state/12) == 3):\n",
        "        optdone = True\n",
        "\n",
        "    return [optact,optdone]\n",
        "\n",
        "\n",
        "'''\n",
        "Now the new action space will contain\n",
        "Primitive Actions: [\"up\", \"right\", \"down\", \"left\"]\n",
        "Options: [\"Away\",\"Close\"]\n",
        "Total Actions :[\"up\", \"right\", \"down\", \"left\", \"Away\", \"Close\"]\n",
        "Corresponding to [0,1,2,3,4,5]\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jmv5c0XoK8GA"
      },
      "source": [
        "# Task 1\n",
        "Complete the code cell below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bh_oghc7Ledh"
      },
      "outputs": [],
      "source": [
        "#Q-Table: (States x Actions) === (env.ns(48) x total actions(6))\n",
        "\n",
        "q_values_SMDP2 = np.zeros((48,6))\n",
        "#Update_Frequency Data structure? Check TODO 4\n",
        "\n",
        "\n",
        "ufd2 = np.zeros((48,6))#Update_Frequency Data structure\n",
        "\n",
        "actions=[0,1,2,3,4,5]\n",
        "# TODO: epsilon-greedy action selection function\n",
        "seed = 36\n",
        "rg = np.random.RandomState(seed)\n",
        "\n",
        "def egreedy_policy(q_values,state,epsilon):\n",
        "    if rg.rand() < epsilon:\n",
        "        return rg.choice([0,1,2,3,4,5])\n",
        "    else:\n",
        "        return np.argmax(q_values[state])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8VJYkqoLqlO"
      },
      "source": [
        "# Task 2\n",
        "Below is an incomplete code cell with the flow of SMDP Q-Learning. Complete the cell and train the agent using SMDP Q-Learning algorithm.\n",
        "Keep the **final Q-table** and **Update Frequency** table handy (You'll need it in TODO 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ok_5eQM7OCTj"
      },
      "outputs": [],
      "source": [
        "#### SMDP Q-Learning\n",
        "\n",
        "# Add parameters you might need here\n",
        "gamma = 0.9\n",
        "alpha = 0.4\n",
        "q_values_SMDP = np.zeros((48,6))\n",
        "ufd1 = np.zeros((48,6))#Update_Frequency Data structure\n",
        "# Iterate over 1000 episodes\n",
        "for _ in range(1000):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    # While episode is not over\n",
        "    while not done:\n",
        "\n",
        "        # Choose action\n",
        "        action = egreedy_policy(q_values_SMDP, state, epsilon=0.1)\n",
        "\n",
        "        # Checking if primitive action\n",
        "        if action < 4:\n",
        "            # Perform regular Q-Learning update for state-action pair\n",
        "\n",
        "            next_state, reward, done,_= env.step(action)\n",
        "            q_values_SMDP[state, action] += alpha*(reward + gamma*np.max([q_values_SMDP[next_state, action] for action in actions]) - q_values_SMDP[state, action])\n",
        "            ufd1[state,action] += 1\n",
        "            state = next_state\n",
        "\n",
        "        # Checking if action chosen is an option\n",
        "        reward_bar = 0\n",
        "        if action == 4: # action => Away option\n",
        "\n",
        "            initial_state = np.copy(state)\n",
        "            optdone = False\n",
        "            count=0\n",
        "            while (optdone == False):\n",
        "\n",
        "                # Think about what this function might do?\n",
        "                optact,_ = Away(env,state)\n",
        "                next_state, reward, done,_ = env.step(optact)\n",
        "\n",
        "\n",
        "                _,optdone = Away(env,next_state)\n",
        "\n",
        "                # Is this formulation right? What is this term?\n",
        "                # Ans: the accumulates return for the entire option\n",
        "                reward_bar = reward_bar +  (gamma**count)*reward\n",
        "                count+=1\n",
        "                # Complete SMDP Q-Learning Update\n",
        "                # Remember SMDP Updates. When & What do you update?\n",
        "                state = next_state\n",
        "\n",
        "            q_values_SMDP[initial_state, action] += alpha*(reward_bar + (gamma**count)*np.max([q_values_SMDP[state, action] for action in actions]) - q_values_SMDP[initial_state, action])\n",
        "            ufd1[initial_state,action] += 1\n",
        "\n",
        "\n",
        "        if action == 5: # action => Close option\n",
        "\n",
        "            initial_state = np.copy(state)\n",
        "            optdone = False\n",
        "            count=0\n",
        "            while (optdone == False):\n",
        "\n",
        "                # Think about what this function might do?\n",
        "                optact,_ = Close(env,state)\n",
        "                next_state, reward, done,_ = env.step(optact)\n",
        "\n",
        "\n",
        "                _,optdone = Close(env,next_state)\n",
        "\n",
        "                # Is this formulation right? What is this term?\n",
        "                # Ans: the accumulates return for the entire option\n",
        "                reward_bar = reward_bar +  (gamma**count)*reward\n",
        "                count+=1\n",
        "                # Complete SMDP Q-Learning Update\n",
        "                # Remember SMDP Updates. When & What do you update?\n",
        "                state = next_state\n",
        "\n",
        "            q_values_SMDP[initial_state, action] += alpha*(reward_bar + (gamma**count)*np.max([q_values_SMDP[state, action] for action in actions]) - q_values_SMDP[initial_state, action])\n",
        "            ufd1[initial_state,action] += 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SQFbRCHWQyO"
      },
      "source": [
        "# Task 3\n",
        "Using the same options and the SMDP code, implement Intra Option Q-Learning (In the code cell below). You *might not* always have to search through options to find the options with similar policies, think about it. Keep the **final Q-table** and **Update Frequency** table handy (You'll need it in TODO 4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "r6A2TdUHWVUN"
      },
      "outputs": [],
      "source": [
        "#### Intra-Option Q-Learning\n",
        "\n",
        "\n",
        "\n",
        "# Add parameters you might need here\n",
        "gamma = 0.9\n",
        "alpha = 0.4\n",
        "\n",
        "# Iterate over 1000 episodes\n",
        "for _ in range(1000):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    # While episode is not over\n",
        "    while not done:\n",
        "\n",
        "        # Choose action\n",
        "        action = egreedy_policy(q_values_SMDP2, state, epsilon=0.1)\n",
        "\n",
        "        # Checking if primitive action\n",
        "        if action < 4:\n",
        "            # Perform regular Q-Learning update for state-action pair\n",
        "\n",
        "            next_state, reward, done,_ = env.step(action)\n",
        "            q_values_SMDP2[state, action] += alpha*(reward + gamma*np.max([q_values_SMDP2[next_state, action] for action in actions]) - q_values_SMDP2[state, action])\n",
        "            ufd2[state,action] += 1\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        # Checking if action chosen is an option\n",
        "        reward_bar = 0\n",
        "        if action == 4: # action => Away option\n",
        "\n",
        "            #initial_state = state\n",
        "            optdone = False\n",
        "            #count=0\n",
        "            while (optdone == False) :\n",
        "\n",
        "                # Think about what this function might do?\n",
        "                optact,_ = Away(env,state)\n",
        "                #\n",
        "                next_state, reward, done,_ = env.step(optact)\n",
        "                _,optdone = Away(env,next_state)\n",
        "\n",
        "                q_values_SMDP2[state, optact] += alpha*(reward + gamma*np.max([q_values_SMDP2[next_state, action] for action in actions]) - q_values_SMDP2[state, optact])\n",
        "                ufd2[state,optact] += 1\n",
        "\n",
        "                if not optdone:\n",
        "                  q_values_SMDP2[state, action] += alpha*(reward + gamma*q_values_SMDP2[next_state, action] - q_values_SMDP2[state, action])\n",
        "                  ufd2[state,action] += 1\n",
        "                else:\n",
        "                  q_values_SMDP2[state, action] += alpha*(reward + gamma*np.max([q_values_SMDP2[next_state, action] for action in actions]) - q_values_SMDP2[state, action])\n",
        "                  ufd2[state,action] += 1\n",
        "\n",
        "\n",
        "                # Complete SMDP Q-Learning Update\n",
        "                # Remember SMDP Updates. When & What do you update?\n",
        "                state = next_state\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        if action == 5: # action => Close option\n",
        "\n",
        "            #initial_state = state\n",
        "            optdone = False\n",
        "            #count=0\n",
        "            while (optdone == False) :\n",
        "\n",
        "                # Think about what this function might do?\n",
        "                optact,_ = Close(env,state)\n",
        "                next_state, reward, done,_ = env.step(optact)\n",
        "                _,optdone = Close(env,next_state)\n",
        "\n",
        "                q_values_SMDP2[state, optact] += alpha*(reward + gamma*np.max([q_values_SMDP2[next_state, action] for action in actions]) - q_values_SMDP2[state, optact])\n",
        "                ufd2[state,optact] += 1\n",
        "\n",
        "                if not optdone:\n",
        "                  q_values_SMDP2[state, action] += alpha*(reward + gamma*q_values_SMDP2[next_state, action] - q_values_SMDP2[state, action])\n",
        "                  ufd2[state,action] += 1\n",
        "                else:\n",
        "                  q_values_SMDP2[state, action] += alpha*(reward + gamma*np.max([q_values_SMDP2[next_state, action] for action in actions]) - q_values_SMDP2[state, action])\n",
        "                  ufd2[state,action] += 1\n",
        "\n",
        "\n",
        "                # Complete SMDP Q-Learning Update\n",
        "                # Remember SMDP Updates. When & What do you update?\n",
        "                state = next_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzUgcwL-VfkO"
      },
      "source": [
        "# Task 4\n",
        "Compare the two Q-Tables and Update Frequencies and provide comments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8mZE74_Vhmg",
        "outputId": "2e80ebd1-4ca4-47be-fd6b-649e3b258d28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          up       right        down      left      Away       Close\n",
            "0  -7.729438   -7.685514   -7.681422 -7.699484 -7.693760   -7.682491\n",
            "1  -7.519931   -7.439467   -7.436725 -7.479699 -7.514018   -7.444857\n",
            "2  -7.201298   -7.161441   -7.166289 -7.365133 -7.250717   -7.165592\n",
            "3  -7.060948   -6.849887   -6.851390 -7.016437 -6.891528   -6.855066\n",
            "4  -6.609679   -6.503400   -6.504559 -6.765017 -6.648498   -6.502854\n",
            "5  -6.176723   -6.117531   -6.119634 -6.537352 -6.312034   -6.120239\n",
            "6  -5.954649   -5.689220   -5.691414 -6.049263 -5.854036   -5.690576\n",
            "7  -5.395808   -5.213450   -5.214202 -5.433794 -5.389702   -5.213467\n",
            "8  -4.795971   -4.684000   -4.684049 -5.009996 -5.007942   -4.683748\n",
            "9  -4.479651   -4.094307   -4.094473 -4.394478 -4.273077   -4.094391\n",
            "10 -3.594209   -3.438714   -3.438715 -4.177258 -3.611374   -3.438828\n",
            "11 -3.195192   -3.047161   -2.709963 -3.561978 -2.786104   -2.709976\n",
            "12 -7.662038   -7.444197   -7.449470 -7.526093 -7.595727   -7.447391\n",
            "13 -7.277369   -7.171653   -7.171223 -7.273409 -7.383913   -7.172108\n",
            "14 -7.053355   -6.859924   -6.860603 -7.169106 -7.243289   -6.860468\n",
            "15 -6.788018   -6.511802   -6.512346 -6.779623 -6.877801   -6.512217\n",
            "16 -6.248781   -6.125228   -6.125326 -6.211379 -6.321114   -6.125115\n",
            "17 -6.072403   -5.695000   -5.695025 -6.067682 -5.906169   -5.695091\n",
            "18 -5.538227   -5.216833   -5.216928 -5.668148 -5.362161   -5.216851\n",
            "19 -5.062393   -4.685464   -4.685506 -5.357580 -5.246825   -4.685483\n",
            "20 -4.535045   -4.095050   -4.095052 -4.492764 -4.838910   -4.095062\n",
            "21 -4.106196   -3.438988   -3.438987 -4.399222 -3.520659   -3.438983\n",
            "22 -2.863692   -2.709998   -2.709999 -3.568895 -2.982318   -2.709999\n",
            "23 -2.393369   -2.050116   -1.900000 -3.201004 -2.584946   -1.900000\n",
            "24 -7.689858   -7.175705   -7.712314 -7.458112 -8.101569   -7.712320\n",
            "25 -7.450856   -6.861894 -106.712280 -7.458126 -7.898010 -106.699772\n",
            "26 -7.172636   -6.513216 -106.712131 -7.175675 -7.695945 -106.711371\n",
            "27 -6.857511   -6.125795 -106.710874 -6.861888 -7.428133 -106.712248\n",
            "28 -6.511685   -5.695328 -106.712289 -6.513198 -7.159570 -106.699827\n",
            "29 -6.125010   -5.217031 -106.705125 -6.125642 -6.834415 -106.700867\n",
            "30 -5.693978   -4.685590 -106.708275 -5.695323 -6.499349 -106.712134\n",
            "31 -5.216550   -4.095100 -106.701225 -5.216843 -6.115752 -106.707989\n",
            "32 -4.684822   -3.439000 -106.626818 -4.685579 -5.690825 -106.681234\n",
            "33 -4.092836   -2.710000 -106.620663 -4.095087 -5.215467 -106.711420\n",
            "34 -3.438989   -1.900000 -106.626429 -3.438988 -4.684439 -106.701249\n",
            "35 -2.709967   -1.899649   -1.000000 -2.709997 -4.094414   -1.000000\n",
            "36 -7.458134 -106.712275   -7.712320 -7.712320 -8.301769   -7.712321\n",
            "37  0.000000    0.000000    0.000000  0.000000  0.000000    0.000000\n",
            "38  0.000000    0.000000    0.000000  0.000000  0.000000    0.000000\n",
            "39  0.000000    0.000000    0.000000  0.000000  0.000000    0.000000\n",
            "40  0.000000    0.000000    0.000000  0.000000  0.000000    0.000000\n",
            "41  0.000000    0.000000    0.000000  0.000000  0.000000    0.000000\n",
            "42  0.000000    0.000000    0.000000  0.000000  0.000000    0.000000\n",
            "43  0.000000    0.000000    0.000000  0.000000  0.000000    0.000000\n",
            "44  0.000000    0.000000    0.000000  0.000000  0.000000    0.000000\n",
            "45  0.000000    0.000000    0.000000  0.000000  0.000000    0.000000\n",
            "46  0.000000    0.000000    0.000000  0.000000  0.000000    0.000000\n",
            "47  0.000000    0.000000    0.000000  0.000000  0.000000    0.000000\n"
          ]
        }
      ],
      "source": [
        "from pandas import DataFrame\n",
        "def table_render(arr):\n",
        "  print(DataFrame(arr,columns=[\"up\", \"right\", \"down\", \"left\", \"Away\", \"Close\"]))\n",
        "\n",
        "table_render(q_values_SMDP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_25qUxcAl0L",
        "outputId": "6618e616-905e-474d-e5bb-529d0baf3558"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          up       right        down      left      Away       Close\n",
            "0  -7.792267   -7.709757   -7.710474 -7.791814 -7.791814   -7.709723\n",
            "1  -7.647197   -7.457365   -7.457592 -7.482925 -7.646183   -7.457277\n",
            "2  -7.387604   -7.175244   -7.175535 -7.500283 -7.349866   -7.175349\n",
            "3  -7.018157   -6.861658   -6.861833 -6.887870 -7.007155   -6.861724\n",
            "4  -6.540943   -6.513052   -6.513181 -6.881067 -6.540192   -6.513109\n",
            "5  -6.398230   -6.125691   -6.125710 -6.523192 -6.392955   -6.125699\n",
            "6  -6.044907   -5.695276   -5.695315 -6.072767 -6.043893   -5.695288\n",
            "7  -5.393733   -5.217012   -5.217022 -5.421611 -5.391594   -5.217016\n",
            "8  -5.078640   -4.685583   -4.685585 -5.056706 -4.984866   -4.685582\n",
            "9  -4.498153   -4.095098   -4.095099 -4.927027 -4.490912   -4.095098\n",
            "10 -3.994070   -3.439000   -3.439000 -4.270104 -3.617607   -3.438999\n",
            "11 -2.902852   -2.786104   -2.710000 -2.949415 -2.786104   -2.710000\n",
            "12 -7.936144   -7.457883   -7.458061 -7.546898 -7.936144   -7.458061\n",
            "13 -7.710566   -7.175695   -7.175703 -7.489496 -7.710363   -7.175697\n",
            "14 -7.455317   -6.861894   -6.861894 -7.110841 -7.454715   -6.861894\n",
            "15 -7.175118   -6.513216   -6.513216 -6.746089 -7.175114   -6.513216\n",
            "16 -6.859984   -6.125795   -6.125795 -6.664280 -6.859742   -6.125795\n",
            "17 -6.511123   -5.695328   -5.695328 -6.021644 -6.511123   -5.695328\n",
            "18 -6.124908   -5.217031   -5.217031 -5.541031 -6.124638   -5.217031\n",
            "19 -5.695260   -4.685590   -4.685590 -5.522642 -5.695198   -4.685590\n",
            "20 -5.216633   -4.095100   -4.095100 -4.959590 -5.216633   -4.095100\n",
            "21 -4.685461   -3.439000   -3.439000 -3.921385 -4.685388   -3.439000\n",
            "22 -4.093956   -2.710000   -2.710000 -3.148989 -4.093811   -2.710000\n",
            "23 -3.438772   -2.514791   -1.900000 -2.634014 -3.438767   -1.900000\n",
            "24 -7.711809   -7.175705   -7.712321 -7.458134 -8.138773   -7.712320\n",
            "25 -7.458094   -6.861894 -106.712321 -7.458133 -7.936056 -106.709692\n",
            "26 -7.175703   -6.513216 -106.712321 -7.175699 -7.701831 -106.700325\n",
            "27 -6.861894   -6.125795 -106.712321 -6.861382 -7.456183 -106.712312\n",
            "28 -6.513215   -5.695328 -106.712320 -6.513214 -7.167657 -106.709889\n",
            "29 -6.125795   -5.217031 -106.712321 -6.125054 -6.853207 -106.711769\n",
            "30 -5.695327   -4.685590 -106.712319 -5.695327 -6.506895 -106.680971\n",
            "31 -5.217031   -4.095100 -106.712276 -5.216890 -6.125274 -106.701226\n",
            "32 -4.685590   -3.439000 -106.712320 -4.685522 -5.692505 -106.705275\n",
            "33 -4.095100   -2.710000 -106.712319 -4.095045 -5.212194 -106.693863\n",
            "34 -3.438996   -1.900000 -106.712277 -3.436701 -4.669204 -106.680574\n",
            "35 -2.710000   -1.900000   -1.000000 -2.709140 -4.092291   -1.000000\n",
            "36 -7.458134 -106.712127   -7.712321 -7.712320 -8.299279   -7.712295\n",
            "37  0.000000    0.000000    0.000000  0.000000  0.000000    0.000000\n",
            "38  0.000000    0.000000    0.000000  0.000000  0.000000    0.000000\n",
            "39  0.000000    0.000000    0.000000  0.000000  0.000000    0.000000\n",
            "40  0.000000    0.000000    0.000000  0.000000  0.000000    0.000000\n",
            "41  0.000000    0.000000    0.000000  0.000000  0.000000    0.000000\n",
            "42  0.000000    0.000000    0.000000  0.000000  0.000000    0.000000\n",
            "43  0.000000    0.000000    0.000000  0.000000  0.000000    0.000000\n",
            "44  0.000000    0.000000    0.000000  0.000000  0.000000    0.000000\n",
            "45  0.000000    0.000000    0.000000  0.000000  0.000000    0.000000\n",
            "46  0.000000    0.000000    0.000000  0.000000  0.000000    0.000000\n",
            "47  0.000000    0.000000    0.000000  0.000000  0.000000    0.000000\n"
          ]
        }
      ],
      "source": [
        "table_render(q_values_SMDP2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH4HhBBVAtvw"
      },
      "source": [
        "Note that both the methods have converged to similar Q-values.\n",
        " The q-values are very low; close to -106 for action 'down' and option 'close' in states 25-35, since it represents the row above the cliff, and the agent has learnt to avoid those actions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIZlYD80Al9I",
        "outputId": "242651c3-d095-4fb7-c6a9-1179350d7030"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        up   right   down  left  Away  Close\n",
            "0     37.0    90.0   59.0  37.0  36.0   20.0\n",
            "1     35.0    89.0   56.0  24.0  35.0   22.0\n",
            "2     32.0    98.0   54.0  23.0  32.0   22.0\n",
            "3     32.0    97.0   50.0  21.0  29.0   22.0\n",
            "4     27.0   100.0   46.0  19.0  28.0   20.0\n",
            "5     24.0    88.0   44.0  19.0  25.0   20.0\n",
            "6     23.0    79.0   42.0  16.0  22.0   19.0\n",
            "7     19.0    71.0   37.0  13.0  19.0   19.0\n",
            "8     16.0    61.0   35.0  12.0  18.0   19.0\n",
            "9     15.0    55.0   34.0  10.0  14.0   19.0\n",
            "10    11.0    34.0   31.0   9.0  11.0   21.0\n",
            "11    10.0     9.0   31.0   8.0   8.0   24.0\n",
            "12    29.0    73.0   27.0  35.0  28.0   27.0\n",
            "13    24.0    88.0   29.0  23.0  25.0   29.0\n",
            "14    23.0    94.0   31.0  21.0  25.0   30.0\n",
            "15    21.0    92.0   30.0  19.0  21.0   29.0\n",
            "16    18.0    91.0   30.0  16.0  19.0   29.0\n",
            "17    17.0    88.0   29.0  16.0  16.0   29.0\n",
            "18    15.0    84.0   30.0  14.0  14.0   27.0\n",
            "19    13.0    74.0   28.0  13.0  13.0   27.0\n",
            "20    12.0    64.0   27.0  10.0  12.0   27.0\n",
            "21    10.0    46.0   28.0  11.0   8.0   27.0\n",
            "22     6.0    37.0   31.0   8.0   6.0   30.0\n",
            "23     5.0     6.0   38.0   8.0   5.0   37.0\n",
            "24   102.0  1339.0   41.0  50.0  50.0   46.0\n",
            "25    68.0  1271.0   29.0  41.0  29.0   18.0\n",
            "26    63.0  1205.0   26.0  37.0  39.0   23.0\n",
            "27    52.0  1149.0   22.0  39.0  31.0   28.0\n",
            "28    49.0  1094.0   30.0  35.0  32.0   18.0\n",
            "29    46.0  1072.0   19.0  30.0  22.0   18.0\n",
            "30    41.0  1026.0   20.0  35.0  26.0   26.0\n",
            "31    37.0  1006.0   18.0  26.0  23.0   20.0\n",
            "32    32.0   993.0   14.0  30.0  22.0   16.0\n",
            "33    24.0   975.0   14.0  28.0  30.0   23.0\n",
            "34    33.0   973.0   14.0  27.0  20.0   18.0\n",
            "35    25.0    18.0  913.0  28.0  19.0   87.0\n",
            "36  1463.0    29.0   61.0  60.0  38.0   66.0\n",
            "37     0.0     0.0    0.0   0.0   0.0    0.0\n",
            "38     0.0     0.0    0.0   0.0   0.0    0.0\n",
            "39     0.0     0.0    0.0   0.0   0.0    0.0\n",
            "40     0.0     0.0    0.0   0.0   0.0    0.0\n",
            "41     0.0     0.0    0.0   0.0   0.0    0.0\n",
            "42     0.0     0.0    0.0   0.0   0.0    0.0\n",
            "43     0.0     0.0    0.0   0.0   0.0    0.0\n",
            "44     0.0     0.0    0.0   0.0   0.0    0.0\n",
            "45     0.0     0.0    0.0   0.0   0.0    0.0\n",
            "46     0.0     0.0    0.0   0.0   0.0    0.0\n",
            "47     0.0     0.0    0.0   0.0   0.0    0.0\n"
          ]
        }
      ],
      "source": [
        "table_render(ufd1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfwMwhe-AmAf",
        "outputId": "15e2ca82-629a-47b0-a51b-271eff550abf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        up   right    down  left  Away  Close\n",
            "0     39.0    69.0    51.0  37.0  37.0   36.0\n",
            "1     42.0    85.0    51.0  25.0  37.0   36.0\n",
            "2     36.0    87.0    51.0  24.0  33.0   38.0\n",
            "3     36.0    93.0    51.0  21.0  30.0   40.0\n",
            "4     31.0    93.0    50.0  20.0  26.0   39.0\n",
            "5     29.0    87.0    45.0  19.0  26.0   36.0\n",
            "6     28.0    83.0    45.0  17.0  24.0   36.0\n",
            "7     23.0    83.0    43.0  14.0  19.0   36.0\n",
            "8     22.0    76.0    40.0  12.0  18.0   35.0\n",
            "9     19.0    64.0    42.0  12.0  16.0   35.0\n",
            "10    17.0    41.0    44.0  10.0  11.0   36.0\n",
            "11    10.0     8.0    57.0   7.0   8.0   52.0\n",
            "12    95.0    70.0    44.0  35.0  94.0   43.0\n",
            "13    68.0    81.0    56.0  24.0  65.0   52.0\n",
            "14    56.0    97.0    59.0  21.0  53.0   58.0\n",
            "15    58.0   101.0    58.0  18.0  52.0   55.0\n",
            "16    51.0   103.0    59.0  19.0  48.0   56.0\n",
            "17    41.0   101.0    61.0  15.0  40.0   58.0\n",
            "18    44.0    93.0    60.0  13.0  39.0   56.0\n",
            "19    45.0    90.0    59.0  14.0  42.0   55.0\n",
            "20    33.0    81.0    59.0  12.0  32.0   56.0\n",
            "21    32.0    68.0    60.0   8.0  29.0   56.0\n",
            "22    24.0    58.0    60.0   7.0  22.0   56.0\n",
            "23    23.0     8.0   128.0   6.0  22.0   97.0\n",
            "24   134.0  1351.0    74.0  62.0  91.0   44.0\n",
            "25    87.0  1269.0    47.0  44.0  54.0   21.0\n",
            "26    83.0  1185.0    51.0  40.0  48.0   18.0\n",
            "27    68.0  1139.0    45.0  31.0  45.0   32.0\n",
            "28    66.0  1080.0    39.0  40.0  39.0   21.0\n",
            "29    57.0  1048.0    47.0  27.0  31.0   24.0\n",
            "30    50.0  1012.0    35.0  38.0  31.0   16.0\n",
            "31    60.0   982.0    29.0  27.0  36.0   18.0\n",
            "32    47.0   954.0    39.0  27.0  28.0   19.0\n",
            "33    38.0   932.0    35.0  26.0  21.0   17.0\n",
            "34    31.0   932.0    29.0  17.0  18.0   16.0\n",
            "35    42.0    36.0  1001.0  17.0  20.0   83.0\n",
            "36  1471.0    26.0    80.0  61.0  69.0   52.0\n",
            "37     0.0     0.0     0.0   0.0   0.0    0.0\n",
            "38     0.0     0.0     0.0   0.0   0.0    0.0\n",
            "39     0.0     0.0     0.0   0.0   0.0    0.0\n",
            "40     0.0     0.0     0.0   0.0   0.0    0.0\n",
            "41     0.0     0.0     0.0   0.0   0.0    0.0\n",
            "42     0.0     0.0     0.0   0.0   0.0    0.0\n",
            "43     0.0     0.0     0.0   0.0   0.0    0.0\n",
            "44     0.0     0.0     0.0   0.0   0.0    0.0\n",
            "45     0.0     0.0     0.0   0.0   0.0    0.0\n",
            "46     0.0     0.0     0.0   0.0   0.0    0.0\n",
            "47     0.0     0.0     0.0   0.0   0.0    0.0\n"
          ]
        }
      ],
      "source": [
        "table_render(ufd2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3x0ow6MAmDI",
        "outputId": "1d1f254d-b651-4906-da43-844b11ae2fad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(21188.0, 23541.0)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.sum(ufd1),np.sum(ufd2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhv4z6D4AmF4",
        "outputId": "533bfaae-07cd-4b9a-c201-1c085db60720"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['up', 'right', 'down', 'left', 'Away', 'Close']\n",
            "[ 2509. 13858.  2098.   871.   850.  1002.]\n",
            "[ 3136. 13766.  2884.   867.  1354.  1534.]\n"
          ]
        }
      ],
      "source": [
        "print([\"up\", \"right\", \"down\", \"left\", \"Away\", \"Close\"])\n",
        "print(np.sum(ufd1,axis=0))\n",
        "print(np.sum(ufd2,axis=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta0-dUlMBVoJ"
      },
      "source": [
        "The frequency of updates in intra-option Q-learning surpasses that of SMDP Q-learning. Pay particular attention to the occurrences of actions such as 'up' and 'down', and options like 'Away' and 'Close'; we will observe a notably higher frequency in intra-option Q-learning, as expected. This disparity arises because in intra-option Q-learning, actions 'up' and 'down' are updated even while executing options, and options themselves are updated at each intermediate step."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "5LBh6_lOVBdN"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
