{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YmPs2cVxOoQZ",
    "outputId": "a29784c9-401c-4b78-92b1-eb22b5f1e70c"
   },
   "outputs": [],
   "source": [
    "# !pip install striprtf\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "g-2WYJovNyjd"
   },
   "outputs": [],
   "source": [
    "# importing system libraries\n",
    "import os\n",
    "from os import walk\n",
    "from string import punctuation\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# importing additional libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from striprtf.striprtf import rtf_to_text\n",
    "plt.rcParams['figure.figsize'] = (20,10)\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9f-XWW6N46X",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Kaggle Dataset\n",
    "# !unzip -o enron-spam.zip -d enron-data\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O25AIJVdN-Dp"
   },
   "outputs": [],
   "source": [
    "def read_test_emails():\n",
    "    testwalk = walk(\"test/\")\n",
    "    emails = []\n",
    "    for root,dr,files in testwalk:\n",
    "#         print(f\"{root},{dr},{files}\")\n",
    "        for file in files:\n",
    "            if \"email\"  in file:\n",
    "    #             print(file)\n",
    "                with open(root + file) as infile:\n",
    "                    content = infile.read()\n",
    "                    text = rtf_to_text(content)\n",
    "                emails.append(text)\n",
    "    return emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Eugo7M1NfNi"
   },
   "outputs": [],
   "source": [
    "pathwalk = walk(\"enron-data/\")\n",
    "SpamData,HamData = [],[]\n",
    "\n",
    "for root,dr,files in pathwalk:\n",
    "    if \"spam\" in str(files):\n",
    "        for file in files:\n",
    "            with open(root + '/' + file,encoding='latin1') as ip:\n",
    "                SpamData.append(\" \".join(ip.readlines()))\n",
    "    if \"ham\" in str(files):\n",
    "        for file in files:\n",
    "            with open(root + '/' + file,encoding='latin1') as ip:\n",
    "                HamData.append(\" \".join(ip.readlines()))\n",
    "\n",
    "SpamData = list(set(SpamData))\n",
    "HamData = list(set(HamData))\n",
    "Data = SpamData + HamData\n",
    "Labels = [\"spam\"]*len(SpamData) + [\"ham\"]*len(HamData)\n",
    "\n",
    "raw_df = pd.DataFrame({\n",
    "    \"email\":Data,\n",
    "    \"label\":Labels\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtiNQ4pPNrRK"
   },
   "source": [
    "# Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YhzLqAAOKrQ"
   },
   "outputs": [],
   "source": [
    "stopWords = stopwords.words('english')\n",
    "vectorizer = CountVectorizer(stop_words=stopWords,min_df=1)\n",
    "test_vectorizer = CountVectorizer(stop_words=stopWords, min_df=1)\n",
    "email = vectorizer.fit_transform(raw_df.email.to_list())\n",
    "label_encoder = sk.preprocessing.LabelEncoder()\n",
    "labels = label_encoder.fit_transform(raw_df.label)\n",
    "X_train,X_test,y_train,y_test = train_test_split(email,labels,train_size=0.8,random_state=42,shuffle=True)\n",
    "X_train = X_train.toarray()\n",
    "X_test = X_test.toarray()\n",
    "y_train = y_train.reshape((-1,1))\n",
    "y_test = y_test.reshape((-1,1))\n",
    "\n",
    "# X_test_ = test_vectorizer.fit_transform(read_test_emails()).toarray()\n",
    "# X_test_mod = np.zeros((X_test_.shape[0],X_train.shape[1]))\n",
    "\n",
    "dict_train = vectorizer.vocabulary_\n",
    "dict_test = test_vectorizer.vocabulary_\n",
    "for key in dict_train.keys():\n",
    "    if key in dict_test.keys():\n",
    "#         print(f\"train:{dict_train[key]},test:{dict_test[key]}\")\n",
    "        X_test_mod[:,dict_train[key]] = X_test_[:,dict_test[key]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uz27bpV5Npwo"
   },
   "outputs": [],
   "source": [
    "# from init import *\n",
    "\n",
    "class NaiveBayes:\n",
    "    def fit(self,X,Y):\n",
    "        spam_count = np.sum(Y)\n",
    "        ham_count = Y.shape[0] - spam_count\n",
    "        P_spam = np.zeros((1,X.shape[1]))\n",
    "        P_ham = np.zeros((1,X.shape[1]))\n",
    "        for i in range(Y.shape[0]):\n",
    "            boolidx = X[i,:] > 0\n",
    "            if Y[i,:] == 1:\n",
    "                P_spam = P_spam + 1*boolidx\n",
    "            else:\n",
    "                P_ham = P_ham + 1*boolidx \n",
    "        P_hat = spam_count/Y.shape[0]\n",
    "        P_spam = P_spam/spam_count\n",
    "        P_ham = P_ham/ham_count\n",
    "        t = np.log((1-P_spam)/(1-P_ham))\n",
    "        self.b = 0\n",
    "        for x in t[0]:\n",
    "            if not np.isnan(x):\n",
    "                self.b += x\n",
    "        self.b += np.log(P_hat/(1 - P_hat))\n",
    "        self.W = np.log((P_spam*(1-P_ham))/(P_ham*(1-P_spam)))\n",
    "    def predict(self,X_test):\n",
    "        valindex = np.where(np.isfinite(self.W[0,:]))[0]\n",
    "        X_test_bool = 1*(X_test > 0)\n",
    "        Y_pred =  X_test_bool[:,valindex] @ self.W[:,valindex].T + self.b\n",
    "        Y_pred[Y_pred > 0] = 1\n",
    "        Y_pred[Y_pred <= 0] = 0\n",
    "        return Y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8WBpyDeDON89"
   },
   "outputs": [],
   "source": [
    "model = NaiveBayes()\n",
    "model.fit(X_train,y_train)\n",
    "Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCehVJKfOR9D"
   },
   "source": [
    "## Validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zGHcttlxNqf_"
   },
   "outputs": [],
   "source": [
    "acc = np.sum(Y_pred == y_test)/Y_pred.shape[0]\n",
    "print(f\"Accuracy : {acc}\")\n",
    "#Predictions of emails in test folder\n",
    "Y_pred = model.predict(X_test_mod)\n",
    "print(Y_pred)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
