\documentclass[solution,addpoints,12pt]{exam}
\printanswers
\usepackage{amsmath,amssymb,graphicx}
\usepackage{centernot}
\usepackage{hyperref}
\newcommand{\RP}{\ensuremath{\mathsf{RP}}}
\newcommand{\expect}[1]{\ensuremath{\mathbb{E}[#1]}}
\newcommand{\dx}{\mathrm{d}x}
\newcommand{\real}{\mathbb{R}}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

%\documentclass[addpoints,11pt,a4paper]{exam}
\renewcommand{\rmdefault}{ppl} % rm
\linespread{1.05}        % Palatino needs more leading
\usepackage[scaled]{helvet} % ss
\usepackage{courier} % tt
\usepackage{eulervm} % a better implementation of the euler package (not in gwTeX)
\normalfont
\usepackage{caption}
\usepackage[T1]{fontenc}
\usepackage{mathrsfs}
\usepackage{comment}
\usepackage{graphicx}
\usepackage{ulem}
\usepackage{paralist}
\usepackage{amsmath}
\usepackage{psfrag}
\usepackage{fullpage}
\usepackage{fancybox}
\usepackage{ifthen}
\usepackage{hyperref}
\usepackage{float}
\usepackage{bbm}
\usepackage{listings}             % Include the listings-package
\newcommand{\red}[1]{\textcolor{red}{#1}}

\lstset{language=Python}
\usepackage{marvosym}
\usepackage[export]{adjustbox}
\extrawidth{1in}
\usepackage{multicol}
\setlength{\columnsep}{.001cm}
\newcommand{\twopartdef}[4]
{
	\left\{
		\begin{array}{ll}
			#1 & \mbox{if } #2 \\
			#3 & \mbox{if } #4
		\end{array}
	\right.
}
\newcommand{\G}{\mathcal{G}}
\newcommand{\fH}{\mathcal{H}}
\newcommand{\M}{\mathcal{M}}

\begin{document}

\hrule
\vspace{3mm}
\noindent 
{\sf IITM-CS5691 : Pattern Recognition and Machine Learning  \hfill Release Date: August 30, 2023}
\\
\noindent 
{\sf Assignment 1 \hfill Due Date : September 14, 2023, 23:59}
%{\sf ~\hfill }
\vspace{3mm}
\hrule
\vspace{3mm}
\noindent{{\sf Roll No:}  \hfill  {\sf Name: Bayes Fisher}}% put your ROLL NO AND NAME HERE

\noindent
{{\sf Collaborators (if any): }} %Names of the collaborators (if any).

\noindent
{{\sf References/sources (if any): 
}} %Reference/source materials, if any.


\vspace{3mm}
\hrule
{\small
\begin{itemize}
\item Use \LaTeX\ to write-up your solutions (in the solution blocks of the source \LaTeX\ file of this assignment), and submit the resulting pdf files (one per question) at Crowdmark by the due date. (Note: {\bf No late submissions} will be allowed, other than one-day late submission with 10\% penalty or four-day late submission with 30\% penalty! Instructions to join Crowdmark and submit your solution to each question within Crowdmark \textbf{TBA} later).
\item For the programming question, please submit your code (rollno.ipynb file and rollno.py file in rollno.zip) directly in moodle, but provide your results/answers (including Jupyter notebook {\bf with output}) in the pdf file you upload to Crowdmark.
\item Collaboration is encouraged, but all write-ups must be done individually and independently, and mention your collaborator(s) if any. Same rules apply for codes written for any programming assignments (i.e., write your own code; we will run plagiarism checks on codes).
\item  If you have referred a book or any other online material or LLMs (Large Language Models like ChatGPT) for obtaining a solution, please cite the source. Again don't copy the source {\it as is} - you may use the source to understand the solution, but write-up the solution in your own words (this also means that you cannot copy-paste the solution from LLMs!). Please be advised that {\it the lesser your reliance on online materials or LLMs} for answering the questions, {\it the more your understanding} of the concepts will be and {\it the more prepared you will be for the course exams}.  
\item Points will be awarded based on how clear, concise and rigorous your solutions are, and how correct your answer is. The weightage of this assignment is 12\% towards the overall course grade. 
\end{itemize}
}
\hrule


\begin{questions} 
\question[8] [{\sc Getting your basics right!}]
\begin{parts}
\part[5] Let a random vector $X$ follow a bivariate Gaussian distribution with mean $\mu = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$ and covariance matrix $\Sigma = \begin{pmatrix} a & b \\ c & d \end{pmatrix}$, i.e., 
$X \sim \mathcal{N} \left( \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} a & b \\ c & d \end{bmatrix} \right)$. Then, use the pdf (probability density function) of $X$ to: 

Find the distribution of (i) $X_2|X_1 = x_1$ and (ii) $X_1|X_2 = x_2$, and use them to (iii) find the permissible values of $a$, $b$, $c$, and $d$. \\
(Hint: You can use the same approach of ``completing the squares'' seen in class).

\part[2] Consider the function $f(x) = x_1^2 + x_2^2 + x_1 x_2$, and a point $v = \begin{bmatrix} 3 \\ 5 \end{bmatrix}$. Find the linear approximation of $f$ around $v$ (i.e., $L_v[f](y)$), and show that the graph of this approximation is a hyperplane in $\real^3$. 
%that passes through the point $[3,5,49]^T$.

\part[1] Which of these statements are true about two random variables $X$ and $Y$ defined on the same probability space?
\begin{enumerate}[(i)]
    \item If $X,Y$ are independent, then $X,Y$ are uncorrelated ($Cov(X,Y)=0$).
    \item If $X,Y$ are uncorrelated, then $X,Y$ are independent. 
    \item If $X,Y$ are uncorrelated and follow a bivariate normal distribution, then $X,Y$ are independent. 
    \item None of the above. 
\end{enumerate}
\end{parts}
%\newpage
%\begin{solution}
%\end{solution}
%\newpage


\question[8] [{\sc Exploring Maximum Likelihood Estimation}]\\
Consider the i.i.d data $\mathbf{X} = \{x_i\}^{n}_{i = 1}$, such that each $x_i \sim \mathcal{N}(\mu, \sigma^2)$. We have seen ML estimates of $\mu, \sigma^2$ in class by setting the gradient to zero.
\begin{parts}
\part[4]
How can you argue that the stationary points so obtained are indeed global maxima of the likelihood function?  

\part[4]
Derive the bias of the MLE of $\mu, \sigma^2$.
\end{parts}
%\newpage
%\begin{solution}
%\end{solution}
%\newpage


\question[8] [{\sc Bayesian Decision Theory}] 
\begin{parts}
\part[4][Optimal Classifier by Pen/Paper]
Let $L$ be the loss matrix defined by $L=\begin{bmatrix}
 0 &1 &2\\
 1 &0 &1\\
 2 &1 &0
\end{bmatrix}$, where $L_{ij}$ indicates the loss for an input x with $i$ being the true class and $j$ the predicted class. Given the data:
\begin{center}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
    \hline
    \textbf{x} &-2.8 & 1.5 & 0.4 &-0.3 & -0.7 & 0.9 & 1.8 & 0.8 & -2.4 & -1.3 & 1.1 & 2.5 & 2.6 & -3.3\\
    \hline
     \textbf{y}& 1 & 3 & 2 & 2 & 1 & 3 & 3 & 2 & 1 & 1 & 2 & 3 & 3 & 1\\
     \hline
\end{tabular}
\end{center}
find the optimal Bayes classifier $h(x)$, and provide its decision boundaries/regions. Assume that the class conditionals are Gaussian distributions with a known variance of 1 and unknown means (to be estimated from the data). 

\part[4] Consider a classification problem in which the loss incurred on mis-classifying an input vector from class $C_k$  as $C_j$ is given by loss matrix entry $L_{kj}$, and for which the loss incurred in selecting the reject option is $\psi$. Find the decision criterion that will give minimum expected loss, and then simplify it for the case of 0-1 loss (i.e., when $L_{kj} = \mathbbm{1}_{k \ne j}$).
\end{parts}
%\newpage
%\begin{solution}
%\end{solution}
%\newpage


\question[8] [{\sc Reverend Bayes decides further!}]
\begin{parts}
\part[2] For a two-class optimal Bayes classifer $h$, the decision region is given by: $R_i = \{ x \in \mathbb{R} : h(x) = C_i\}$. Is $R_1$ always a single interval (based on a single cutoff separating the $C_1$ and $C_2$ class) or can $R_1$ be composed of more than one discontiguous interval? If yes for latter, give an example by plotting the pdfs $p(x,C_1)$ and $p(x,C_2)$ against $x$.  

\part[2] For a binary classifer $h$, let 
$L=\begin{bmatrix}
 p & q\\
 r & s
\end{bmatrix}$ be the loss matrix; and 
$C_{\mathrm{train}}=\begin{bmatrix}
 100 & 10\\
 20 & 120
\end{bmatrix}$, and 
$C_{\mathrm{test}}=\begin{bmatrix}
 90 & 45\\
 30 & 85
\end{bmatrix}$ be the confusion matrix when $h$ is applied on the training and test data respectively. All three matrices have ground-truth classes $t$ along the rows and predictions $h$ along the columns in the same order for the two classes. Express your estimate of the expected loss of $h$ in terms of $p$ to $s$ above. 

\part[4] Consider the dataset introduced in the table below, where the task is to predict whether a person is ill. We use a representation based on three features per subject to describe an individual person. These features are ``running nose (N)”, ``coughing (C)”, and ``reddened skin (R)”, each of which can take the value true (`+’) or false (`–’). (i) Classify the data point ($d_7: N=-$, $C=+$, $R=-$) using a Naive Bayes classifier. As part of your solution, also write down the (ii) Naive Bayes assumption and (iii) Naive Bayes classifier, along with (iv) which distribution's MLE formula you used to estimate the class conditionals. 
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{prml_asst1_fig.NB.png}
\end{figure}
\end{parts}
%\newpage
%\begin{solution}
%\end{solution}
%\newpage


\question[16] [{\sc Let's roll up your coding sleeves...}]
\red{(Note: You should follow instructions in the preamble on how to submit notebook with output/results, as well as the code source files, to get full credit for this programming question.)}\\
You are supposed to build Bayesian classifiers that model each class using multivariate Gaussian density functions for the datasets assigned to you (under assumptions below and employing MLE approach to estimate class prior/conditional densities). This assignment is focused on handling and analyzing data using interpretable classification models, rather than aiming solely for the best classification accuracy. 
    
Build Bayesian models for the given case numbers (you may refer to the Chapter 2 of the book ``Pattern Classification" by David G. Stork, Peter E. Hart, and Richard O. Duda):
\begin{enumerate}
    \item[Case 1:] Bayes classifier with the same Covariance matrix for all classes.
    \item[Case 2:] Bayes classifier with different Covariance matrix across classes.
    \item[Case 3:] Naive Bayes classifier with the Covariance matrix $S = \sigma^2 \mathbf{I}$ same for all classes.
    \item[Case 4:] Naive Bayes classifier with $S$ of the above form, but being different across classes.
\end{enumerate}
    
Refer to the provided dataset for each group, which can be found  \href{https://drive.google.com/drive/folders/1NmqA9lkxXayVaCzEfRgSxSxCYSa0LEZu?usp=sharing}{here}. Each dataset includes 2D feature vectors and their corresponding class labels. There are two different datasets available:
\begin{enumerate}
    \item Linearly separable data.
    \item Non-linearly separable data.
\end{enumerate}
There are 41 folders in each dataset, but you need to look at only one folder -- {\bf the folder number assigned to you} being $RollNo\%41 + 1$.
    
\textbf{Plots/answers Required}:
For your assignment, you need to provide the following plots/answers (refer to the "Sample Plots" folder: \href{https://drive.google.com/drive/folders/1jhauePXVWVnmUEkmZeutuhlzosTRz1sU?usp=sharing}{link}):
    
\begin{parts}
    \part[4] The plot of Gaussian pdfs for all classes estimated using the train data (train.txt). (4 Cases $\times$ 2 Datasets = 8 plots in one page)
    \part[4] The classifiers, specifically their decision boundary/surface as a 2D plot along with training points marked in the plot (again 8 plots in one page).
    \part[1] Report the error rates for the above classifiers (four classifiers on the two datasets as a 4 × 2 table, with appropriately named rows and columns).
    \part[1] Answer briefly on whether we can use the most general ``Case 2'' for all datasets? If not, answer when a simpler model like ``Case 1'' is preferable over ``Case 2''? 
    \part[6] Ensure that the properly running code files that generates the above plots, etc., are submitted according to the detailed instructions in the preamble.
\end{parts}
    
\textbf{(Not)Allowed Libraries:}
You are not allowed to use any inbuilt functions for building the model or classification using the model. However, you can use inbuilt functions/libraries for plotting and other purposes.
%\newpage
%\begin{solution}
%\end{solution}
%\newpage
\end{questions}
\end{document}

