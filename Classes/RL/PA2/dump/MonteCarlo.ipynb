{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import gymnasium as gym\n",
    "import flax\n",
    "from flax.training.train_state import TrainState\n",
    "import optax\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.001\n",
    "GAMMA = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_rewards(rewards, window_size=10):\n",
    "    smoothed_rewards = np.zeros_like(rewards)\n",
    "    for i in range(len(rewards)):\n",
    "        window_start = max(0, i - window_size // 2)\n",
    "        window_end = min(len(rewards), i + window_size // 2 + 1)\n",
    "        smoothed_rewards[i] = np.mean(rewards[window_start:window_end])\n",
    "    return smoothed_rewards\n",
    "\n",
    "\n",
    "def plot_data(mean, std):\n",
    "    x = range(len(mean))\n",
    "\n",
    "    plt.plot(x, mean, color='blue', label='Mean')\n",
    "    plt.plot(x, smooth_rewards(mean), color='orange', label='smoothed')\n",
    "    plt.fill_between(x, mean - std, mean + std, color='blue',\n",
    "                     alpha=0.3, label='Mean ± Std')\n",
    "\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Rewards')\n",
    "    plt.title('Mean with Standard Deviation')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(flax.linen.Module):\n",
    "    action_dim: int\n",
    "\n",
    "    @flax.linen.compact\n",
    "    def __call__(self, x):\n",
    "        x = flax.linen.Dense(16)(x)\n",
    "        x = flax.linen.leaky_relu(x)\n",
    "        x = flax.linen.Dense(self.action_dim)(x)\n",
    "        x = flax.linen.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BaselineNetwork(flax.linen.Module):\n",
    "\n",
    "    @flax.linen.compact\n",
    "    def __call__(self, x):\n",
    "        x = flax.linen.Dense(16)(x)\n",
    "        x = flax.linen.leaky_relu(x)\n",
    "        x = flax.linen.Dense(1)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_Reinforce:\n",
    "    def __init__(self, env, num_actions, observation_shape, seed=0):\n",
    "        self.seed = seed\n",
    "        self.rng = jax.random.PRNGKey(seed)\n",
    "        self.num_actions = num_actions\n",
    "        self.observation_shape = observation_shape\n",
    "        self.env = env\n",
    "\n",
    "        self.policy = PolicyNetwork(num_actions)\n",
    "        self.policy_state = TrainState.create(\n",
    "            apply_fn=self.policy.apply,\n",
    "            params=self.policy.init(self.rng, jnp.ones(observation_shape)),\n",
    "            tx=optax.adam(learning_rate=ALPHA),\n",
    "        )\n",
    "        self.policy.apply = jax.jit(self.policy.apply)\n",
    "        # print(self.policy.tabulate(self.rng, jnp.ones(\n",
    "        #     self.observation_shape)))\n",
    "\n",
    "    def sample(self, state):\n",
    "        probs = self.policy.apply(self.policy_state.params, state)[0]\n",
    "        return probs\n",
    "\n",
    "    # @functools.partial(jax.jit, static_argnums=(0,))\n",
    "    def update(self, states, actions, discounted_rewards):\n",
    "        @jax.jit\n",
    "        def log_prob_loss(params):\n",
    "            probs = self.policy.apply(params, states)\n",
    "            log_probs = jnp.log(probs)\n",
    "            actions_new = jax.nn.one_hot(actions, num_classes=self.num_actions)\n",
    "            prob_reduce = -jnp.sum(log_probs*actions_new, axis=1)\n",
    "            loss = jnp.mean(prob_reduce*discounted_rewards)\n",
    "            return loss\n",
    "\n",
    "        loss, grads = jax.value_and_grad(\n",
    "            log_prob_loss)(self.policy_state.params)\n",
    "        self.policy_state = self.policy_state.apply_gradients(grads=grads)\n",
    "        return loss\n",
    "\n",
    "    def train_single_step(self):\n",
    "        state = self.env.reset(seed=self.seed)[0]\n",
    "        key = self.rng\n",
    "\n",
    "        episode_rewards = []\n",
    "        episode_states = []\n",
    "        episode_actions = []\n",
    "\n",
    "        for _ in range(500):\n",
    "            _, key = jax.random.split(key=key)\n",
    "            probs = self.sample(np.expand_dims(state, axis=0))\n",
    "            action = np.random.choice(self.num_actions, p=np.array(probs))\n",
    "            episode_actions.append(action)\n",
    "            episode_states.append(state)\n",
    "            next_state, reward, done, truncated, info = self.env.step(action)\n",
    "            episode_rewards.append(reward)\n",
    "            state = next_state\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "        discounted_rewards = jnp.array([sum(reward * (GAMMA ** t) for t, reward in enumerate(episode_rewards[start:]))\n",
    "                                        for start in range(len(episode_rewards))])\n",
    "        gamma_t = jnp.array([sum(GAMMA ** t for t, reward in enumerate(episode_rewards[start:]))\n",
    "                             for start in range(len(episode_rewards))])\n",
    "        discounted_rewards = (\n",
    "            discounted_rewards-discounted_rewards.mean())/(discounted_rewards.std()+1e-8)\n",
    "        episode_states = jnp.array(episode_states)\n",
    "        episode_actions = jnp.array(episode_actions)\n",
    "        loss = self.update(episode_states, episode_actions,\n",
    "                           discounted_rewards*gamma_t)\n",
    "        return loss, np.sum(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_Baseline:\n",
    "    def __init__(self, env, num_actions, observation_shape, seed=0):\n",
    "        self.seed = seed\n",
    "        self.rng = jax.random.PRNGKey(seed)\n",
    "        self.num_actions = num_actions\n",
    "        self.observation_shape = observation_shape\n",
    "        self.env = env\n",
    "\n",
    "        self.policy = PolicyNetwork(num_actions)\n",
    "        self.policy_state = TrainState.create(\n",
    "            apply_fn=self.policy.apply,\n",
    "            params=self.policy.init(self.rng, jnp.ones(observation_shape)),\n",
    "            tx=optax.adam(learning_rate=ALPHA),\n",
    "        )\n",
    "        self.policy.apply = jax.jit(self.policy.apply)\n",
    "\n",
    "        self.baseline = BaselineNetwork()\n",
    "        self.baseline_state = TrainState.create(\n",
    "            apply_fn=self.baseline.apply,\n",
    "            params=self.baseline.init(self.rng, jnp.zeros(observation_shape)),\n",
    "            tx=optax.adam(learning_rate=ALPHA),\n",
    "        )\n",
    "        self.baseline.apply = jax.jit(self.baseline.apply)\n",
    "        # print(self.policy.tabulate(self.rng, jnp.ones(\n",
    "        #     self.observation_shape)))\n",
    "\n",
    "    def sample(self, state):\n",
    "        probs = self.policy.apply(self.policy_state.params, state)[0]\n",
    "        return probs\n",
    "\n",
    "    # @functools.partial(jax.jit, static_argnums=(0,))\n",
    "    def update(self, states, actions, discounted_rewards, gamma_t):\n",
    "        @jax.jit\n",
    "        def mse_loss(params):\n",
    "            v_s = self.baseline.apply(params, states)\n",
    "            delta = jnp.subtract(discounted_rewards, jnp.reshape(v_s, (-1,)))\n",
    "            loss = jnp.mean(0.5*jnp.square(delta))\n",
    "            return loss, delta\n",
    "\n",
    "        (loss_baseline, delta), grads_baseline = jax.value_and_grad(\n",
    "            mse_loss, has_aux=True)(self.baseline_state.params)\n",
    "\n",
    "        @jax.jit\n",
    "        def log_prob_loss(params):\n",
    "            probs = self.policy.apply(params, states)\n",
    "            log_probs = jnp.log(probs)\n",
    "            actions_new = jax.nn.one_hot(actions, num_classes=self.num_actions)\n",
    "            prob_reduce = -jnp.sum(log_probs*actions_new, axis=1)\n",
    "            loss = jnp.mean(prob_reduce*delta*gamma_t)\n",
    "            return loss\n",
    "\n",
    "        loss_policy, grads_policy = jax.value_and_grad(\n",
    "            log_prob_loss)(self.policy_state.params)\n",
    "\n",
    "        self.baseline_state = self.baseline_state.apply_gradients(\n",
    "            grads=grads_baseline)\n",
    "        self.policy_state = self.policy_state.apply_gradients(\n",
    "            grads=grads_policy)\n",
    "        return loss_policy+loss_baseline\n",
    "\n",
    "    def train_single_step(self):\n",
    "        state = self.env.reset(seed=self.seed)[0]\n",
    "        key = self.rng\n",
    "\n",
    "        episode_rewards = []\n",
    "        episode_states = []\n",
    "        episode_actions = []\n",
    "\n",
    "        for _ in range(500):\n",
    "            _, key = jax.random.split(key=key)\n",
    "            probs = self.sample(np.expand_dims(state, axis=0))\n",
    "            action = np.random.choice(self.num_actions, p=np.array(probs))\n",
    "            episode_actions.append(action)\n",
    "            episode_states.append(state)\n",
    "            next_state, reward, done, truncated, info = self.env.step(action)\n",
    "            episode_rewards.append(reward)\n",
    "            state = next_state\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "        discounted_rewards = jnp.array([sum(reward * (GAMMA ** t) for t, reward in enumerate(episode_rewards[start:]))\n",
    "                                        for start in range(len(episode_rewards))])\n",
    "        gamma_t = jnp.array([sum(GAMMA ** t for t, reward in enumerate(episode_rewards[start:]))\n",
    "                             for start in range(len(episode_rewards))])\n",
    "        discounted_rewards = (\n",
    "            discounted_rewards-discounted_rewards.mean())/(discounted_rewards.std()+1e-8)\n",
    "        episode_states = jnp.array(episode_states)\n",
    "        episode_actions = jnp.array(episode_actions)\n",
    "        loss = self.update(episode_states, episode_actions,\n",
    "                           discounted_rewards, gamma_t)\n",
    "        return loss, np.sum(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulation:\n",
    "    def __init__(self, env_name, algorithm) -> None:\n",
    "        self.env_name = env_name\n",
    "        self.algorithm = algorithm\n",
    "        self.env = gym.make(self.env_name)\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        self.observation_shape = self.env.observation_space.shape\n",
    "\n",
    "    def train(self, episodes=1000):\n",
    "        self.losses, self.rewards = np.zeros(\n",
    "            (5, episodes)), np.zeros((5, episodes))\n",
    "\n",
    "        for seed in range(5):\n",
    "            self.algo = self.algorithm(\n",
    "                self.env, self.num_actions, self.observation_shape, seed=seed)\n",
    "            for ep in tqdm.tqdm(range(1, episodes+1)):\n",
    "                loss, reward = self.algo.train_single_step()\n",
    "                self.losses[seed][ep-1] = loss\n",
    "                self.rewards[seed][ep-1] = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelDQN_MeanAdvantage:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class DuelDQN_MaxAdvantage:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [03:36<00:00,  4.63it/s]\n",
      "100%|██████████| 1000/1000 [02:26<00:00,  6.85it/s]\n",
      "100%|██████████| 1000/1000 [02:25<00:00,  6.85it/s]\n",
      "100%|██████████| 1000/1000 [02:29<00:00,  6.69it/s]\n",
      "100%|██████████| 1000/1000 [02:35<00:00,  6.44it/s]\n"
     ]
    }
   ],
   "source": [
    "cartpole_reinforce = Simulation('CartPole-v1', algorithm=MC_Reinforce)\n",
    "cartpole_reinforce.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [05:22<00:00,  3.10it/s]\n",
      "100%|██████████| 1000/1000 [04:04<00:00,  4.09it/s]\n",
      "100%|██████████| 1000/1000 [04:50<00:00,  3.45it/s]\n",
      "100%|██████████| 1000/1000 [05:00<00:00,  3.33it/s]\n",
      "100%|██████████| 1000/1000 [05:08<00:00,  3.24it/s]\n"
     ]
    }
   ],
   "source": [
    "cartpole_baseline = Simulation('CartPole-v1', algorithm=MC_Baseline)\n",
    "cartpole_baseline.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [04:33<00:00,  3.65it/s]\n",
      "100%|██████████| 1000/1000 [08:30<00:00,  1.96it/s]\n",
      "100%|██████████| 1000/1000 [08:23<00:00,  1.99it/s]\n",
      "100%|██████████| 1000/1000 [05:47<00:00,  2.87it/s]\n",
      " 51%|█████     | 509/1000 [04:22<04:03,  2.02it/s]E0402 00:31:37.081436  358319 pjrt_stream_executor_client.cc:2804] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Failed to get module function:CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      " 51%|█████     | 509/1000 [04:22<04:13,  1.94it/s]\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "RESOURCE_EXHAUSTED: Failed to get module function:CUDA_ERROR_OUT_OF_MEMORY: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJaxStackTraceBeforeTransformation\u001b[0m         Traceback (most recent call last)",
      "File \u001b[0;32m<frozen runpy>:198\u001b[0m, in \u001b[0;36m_run_module_as_main\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<frozen runpy>:88\u001b[0m, in \u001b[0;36m_run_code\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/ipykernel_launcher.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mipykernel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m kernelapp \u001b[38;5;28;01mas\u001b[39;00m app\n\u001b[0;32m---> 17\u001b[0m app\u001b[38;5;241m.\u001b[39mlaunch_new_instance()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/traitlets/config/application.py:1075\u001b[0m, in \u001b[0;36mlaunch_instance\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1074\u001b[0m app\u001b[38;5;241m.\u001b[39minitialize(argv)\n\u001b[0;32m-> 1075\u001b[0m app\u001b[38;5;241m.\u001b[39mstart()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/ipykernel/kernelapp.py:739\u001b[0m, in \u001b[0;36mstart\u001b[0;34m()\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 739\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mio_loop\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tornado/platform/asyncio.py:205\u001b[0m, in \u001b[0;36mstart\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masyncio_loop\u001b[38;5;241m.\u001b[39mrun_forever()\n",
      "File \u001b[0;32m/usr/lib/python3.11/asyncio/base_events.py:607\u001b[0m, in \u001b[0;36mrun_forever\u001b[0;34m()\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_once()\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n",
      "File \u001b[0;32m/usr/lib/python3.11/asyncio/base_events.py:1922\u001b[0m, in \u001b[0;36m_run_once\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1921\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1922\u001b[0m         handle\u001b[38;5;241m.\u001b[39m_run()\n\u001b[1;32m   1923\u001b[0m handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/asyncio/events.py:80\u001b[0m, in \u001b[0;36m_run\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mSystemExit\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/ipykernel/kernelbase.py:542\u001b[0m, in \u001b[0;36mdispatch_queue\u001b[0;34m()\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_one()\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/ipykernel/kernelbase.py:531\u001b[0m, in \u001b[0;36mprocess_one\u001b[0;34m()\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 531\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m dispatch(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/ipykernel/kernelbase.py:437\u001b[0m, in \u001b[0;36mdispatch_shell\u001b[0;34m()\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[0;32m--> 437\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m result\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/ipykernel/ipkernel.py:359\u001b[0m, in \u001b[0;36mexecute_request\u001b[0;34m()\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_associate_new_top_level_threads_with(parent_header)\n\u001b[0;32m--> 359\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mexecute_request(stream, ident, parent)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/ipykernel/kernelbase.py:775\u001b[0m, in \u001b[0;36mexecute_request\u001b[0;34m()\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(reply_content):\n\u001b[0;32m--> 775\u001b[0m     reply_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m reply_content\n\u001b[1;32m    777\u001b[0m \u001b[38;5;66;03m# Flush output before sending the reply.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/ipykernel/ipkernel.py:446\u001b[0m, in \u001b[0;36mdo_execute\u001b[0;34m()\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accepts_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcell_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 446\u001b[0m     res \u001b[38;5;241m=\u001b[39m shell\u001b[38;5;241m.\u001b[39mrun_cell(\n\u001b[1;32m    447\u001b[0m         code,\n\u001b[1;32m    448\u001b[0m         store_history\u001b[38;5;241m=\u001b[39mstore_history,\n\u001b[1;32m    449\u001b[0m         silent\u001b[38;5;241m=\u001b[39msilent,\n\u001b[1;32m    450\u001b[0m         cell_id\u001b[38;5;241m=\u001b[39mcell_id,\n\u001b[1;32m    451\u001b[0m     )\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/ipykernel/zmqshell.py:549\u001b[0m, in \u001b[0;36mrun_cell\u001b[0;34m()\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrun_cell(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3051\u001b[0m, in \u001b[0;36mrun_cell\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3050\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3051\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_cell(\n\u001b[1;32m   3052\u001b[0m         raw_cell, store_history, silent, shell_futures, cell_id\n\u001b[1;32m   3053\u001b[0m     )\n\u001b[1;32m   3054\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3106\u001b[0m, in \u001b[0;36m_run_cell\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3105\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3106\u001b[0m     result \u001b[38;5;241m=\u001b[39m runner(coro)\n\u001b[1;32m   3107\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/IPython/core/async_helpers.py:129\u001b[0m, in \u001b[0;36m_pseudo_sync_runner\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3311\u001b[0m, in \u001b[0;36mrun_cell_async\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3308\u001b[0m interactivity \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m silent \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mast_node_interactivity\n\u001b[0;32m-> 3311\u001b[0m has_raised \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_ast_nodes(code_ast\u001b[38;5;241m.\u001b[39mbody, cell_name,\n\u001b[1;32m   3312\u001b[0m        interactivity\u001b[38;5;241m=\u001b[39minteractivity, compiler\u001b[38;5;241m=\u001b[39mcompiler, result\u001b[38;5;241m=\u001b[39mresult)\n\u001b[1;32m   3314\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_execution_succeeded \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m has_raised\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3493\u001b[0m, in \u001b[0;36mrun_ast_nodes\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3492\u001b[0m     asy \u001b[38;5;241m=\u001b[39m compare(code)\n\u001b[0;32m-> 3493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_code(code, result, async_\u001b[38;5;241m=\u001b[39masy):\n\u001b[1;32m   3494\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3553\u001b[0m, in \u001b[0;36mrun_code\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3552\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3553\u001b[0m         exec(code_obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_global_ns, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns)\n\u001b[1;32m   3554\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   3555\u001b[0m     \u001b[38;5;66;03m# Reset our crash handler in place\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m acrobot_reinforce \u001b[38;5;241m=\u001b[39m Simulation(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAcrobot-v1\u001b[39m\u001b[38;5;124m'\u001b[39m,algorithm\u001b[38;5;241m=\u001b[39mMC_Reinforce)\n\u001b[0;32m----> 2\u001b[0m acrobot_reinforce\u001b[38;5;241m.\u001b[39mtrain()\n",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, episodes\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m---> 17\u001b[0m     loss, reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgo\u001b[38;5;241m.\u001b[39mtrain_single_step()\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses[seed][ep\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m loss\n",
      "Cell \u001b[0;32mIn[5], line 67\u001b[0m, in \u001b[0;36mtrain_single_step\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m episode_actions \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray(episode_actions)\n\u001b[0;32m---> 67\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(episode_states, episode_actions,\n\u001b[1;32m     68\u001b[0m                    discounted_rewards\u001b[38;5;241m*\u001b[39mgamma_t)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss, np\u001b[38;5;241m.\u001b[39msum(episode_rewards)\n",
      "Cell \u001b[0;32mIn[5], line 34\u001b[0m, in \u001b[0;36mupdate\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[0;32m---> 34\u001b[0m loss, grads \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvalue_and_grad(\n\u001b[1;32m     35\u001b[0m     log_prob_loss)(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_state\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_state\u001b[38;5;241m.\u001b[39mapply_gradients(grads\u001b[38;5;241m=\u001b[39mgrads)\n",
      "\u001b[0;31mJaxStackTraceBeforeTransformation\u001b[0m: jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: Failed to get module function:CUDA_ERROR_OUT_OF_MEMORY: out of memory\n\nThe preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.\n\n--------------------",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m acrobot_reinforce \u001b[38;5;241m=\u001b[39m Simulation(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAcrobot-v1\u001b[39m\u001b[38;5;124m'\u001b[39m,algorithm\u001b[38;5;241m=\u001b[39mMC_Reinforce)\n\u001b[0;32m----> 2\u001b[0m \u001b[43macrobot_reinforce\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m, in \u001b[0;36mSimulation.train\u001b[0;34m(self, episodes)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm(\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_shape, seed\u001b[38;5;241m=\u001b[39mseed)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, episodes\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m---> 17\u001b[0m     loss, reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_single_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses[seed][ep\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards[seed][ep\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[0;32mIn[5], line 67\u001b[0m, in \u001b[0;36mMC_Reinforce.train_single_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m episode_states \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray(episode_states)\n\u001b[1;32m     66\u001b[0m episode_actions \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray(episode_actions)\n\u001b[0;32m---> 67\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisode_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mdiscounted_rewards\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgamma_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss, np\u001b[38;5;241m.\u001b[39msum(episode_rewards)\n",
      "Cell \u001b[0;32mIn[5], line 34\u001b[0m, in \u001b[0;36mMC_Reinforce.update\u001b[0;34m(self, states, actions, discounted_rewards)\u001b[0m\n\u001b[1;32m     31\u001b[0m     loss \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mmean(prob_reduce\u001b[38;5;241m*\u001b[39mdiscounted_rewards)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[0;32m---> 34\u001b[0m loss, grads \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_and_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_prob_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_state\u001b[38;5;241m.\u001b[39mapply_gradients(grads\u001b[38;5;241m=\u001b[39mgrads)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "    \u001b[0;31m[... skipping hidden 15 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/jax/_src/interpreters/pxla.py:1209\u001b[0m, in \u001b[0;36mExecuteReplicated.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_token_bufs(result_token_bufs, sharded_runtime_token)\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxla_executable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_sharded\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_bufs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch\u001b[38;5;241m.\u001b[39mneeds_check_special():\n\u001b[1;32m   1211\u001b[0m   out_arrays \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mdisassemble_into_single_device_arrays()\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: RESOURCE_EXHAUSTED: Failed to get module function:CUDA_ERROR_OUT_OF_MEMORY: out of memory"
     ]
    }
   ],
   "source": [
    "acrobot_reinforce = Simulation('Acrobot-v1',algorithm=MC_Reinforce)\n",
    "acrobot_reinforce.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [05:47<00:00,  2.88it/s]\n",
      "100%|██████████| 1000/1000 [10:48<00:00,  1.54it/s]\n",
      "100%|██████████| 1000/1000 [10:55<00:00,  1.53it/s]\n",
      "100%|██████████| 1000/1000 [07:16<00:00,  2.29it/s]\n",
      "100%|██████████| 1000/1000 [11:01<00:00,  1.51it/s]\n"
     ]
    }
   ],
   "source": [
    "acrobot_baseline = Simulation('Acrobot-v1',algorithm=MC_Baseline)\n",
    "acrobot_baseline.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_cartpole_reinforce = cartpole_reinforce.rewards\n",
    "rewards_cartpole_baseline = cartpole_baseline.rewards\n",
    "rewards_acrobot_reinforce = acrobot_reinforce.rewards\n",
    "rewards_acrobot_baseline = acrobot_baseline.rewards\n",
    "\n",
    "\n",
    "mean_rcr = np.mean(rewards_cartpole_reinforce, axis=0)\n",
    "mean_rcb = np.mean(rewards_cartpole_baseline, axis=0)\n",
    "mean_rar = np.mean(rewards_acrobot_reinforce, axis=0)\n",
    "mean_rab = np.mean(rewards_acrobot_baseline, axis=0)\n",
    "mean_mat = [mean_rcr, mean_rcb, mean_rar, mean_rab]\n",
    "\n",
    "std_rcr = np.std(rewards_cartpole_reinforce, axis=0)\n",
    "std_rcb = np.std(rewards_cartpole_baseline, axis=0)\n",
    "std_rar = np.std(rewards_acrobot_reinforce, axis=0)\n",
    "std_rab = np.std(rewards_acrobot_baseline, axis=0)\n",
    "std_mat = [std_rcr, std_rcb, std_rar, std_rab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcartpole_reinforce\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcartpole_baseline\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      2\u001b[0m          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124macrobot_reinforce\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124macrobot_baseline\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m12\u001b[39m))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "names = ['cartpole_reinforce', 'cartpole_baseline',\n",
    "         'acrobot_reinforce', 'acrobot_baseline']\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12, 12))\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        mean = mean_mat[i*2+j]\n",
    "        std = std_mat[i*2+j]\n",
    "        x = range(len(mean))\n",
    "        ax[i][j].plot(x, mean, color='blue', label='Mean')\n",
    "        ax[i][j].plot(x, smooth_rewards(mean),\n",
    "                      color='orange', label='smoothed')\n",
    "        ax[i][j].fill_between(x, mean - std, mean + std, color='blue',\n",
    "                              alpha=0.3, label='Mean ± Std')\n",
    "        ax[i][j].set_xlabel('Steps')\n",
    "        ax[i][j].set_ylabel('Rewards')\n",
    "        ax[i][j].set_title(names[i*2+j])\n",
    "        ax[i][j].legend()\n",
    "        ax[i][j].grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
